{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo  import MongoClient\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import string\n",
    "import scipy.stats as st\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition, naive_bayes, preprocessing, model_selection, metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import model_selection\n",
    "from tqdm.notebook import tqdm\n",
    "#! pip install scikit-surprise\n",
    "from surprise import NormalPredictor, BaselineOnly, SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données AVIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(host=\"localhost\", port=27017)\n",
    "db = client[\"PLDAC\"]\n",
    "collection = db[\"avis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avis = pd.DataFrame(list(collection.find())).loc[:,[\"author\",\"title\",\"note\"]]\n",
    "df_avis.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avis['author'].value_counts().describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppression des autheurs qui ont noté moins de 5 jeux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Taille du df avant {len(df_avis)}\")\n",
    "# Taille du df avant 246524\n",
    "\n",
    "authors = df_avis['author'].value_counts()\n",
    "authors = authors[authors >= 10].index.to_list()\n",
    "\n",
    "df_avis_k = df_avis[df_avis['author'].isin(authors)]\n",
    "print(f\"Taille du df après {len(df_avis_k)}\")\n",
    "# Taille du df après 5925"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Train split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = model_selection.train_test_split(df_avis_k, test_size=0.2, random_state=0)\n",
    "df_avis.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Recsys **mean** baselines implementations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = X_train[\"note\"].mean()\n",
    "USER_MEANS = X_train.groupby(\"author\")[\"note\"].mean()\n",
    "ITEM_MEANS = X_train.groupby(\"title\")[\"note\"].mean()\n",
    "\n",
    "\n",
    "def mean_rating_pred(user_item):\n",
    "    user = user_item[\"author\"]\n",
    "    item = user_item[\"title\"]\n",
    "    \n",
    "    return MEAN\n",
    "\n",
    "def user_mean_rating_pred(user_item):\n",
    "    user = user_item[\"author\"]\n",
    "    item = user_item[\"title\"]\n",
    "    \n",
    "    return USER_MEANS.get(user,default=MEAN)\n",
    "\n",
    "def item_mean_rating_pred(user_item):\n",
    "    user = user_item[\"author\"]\n",
    "    item = user_item[\"title\"]\n",
    "    \n",
    "    return ITEM_MEANS.get(item,default=MEAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the rating prediction columns\n",
    "X_test[\"mean_prediction\"] = X_test[[\"author\",\"title\"]].apply(mean_rating_pred,axis=1)\n",
    "X_test[\"muser_prediction\"] = X_test[[\"author\",\"title\"]].apply(user_mean_rating_pred,axis=1) \n",
    "X_test[\"mitem_prediction\"] = X_test[[\"author\",\"title\"]].apply(item_mean_rating_pred,axis=1) \n",
    "\n",
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def mae(predictions,truth):\n",
    "    return ((predictions-truth).abs()).mean()\n",
    "\n",
    "def mse(predictions,truth):\n",
    "    return ((predictions - truth)**2).mean()\n",
    "\n",
    "def rmse(predictions,truth):\n",
    "    return sqrt(mse(predictions,truth))\n",
    "\n",
    "\n",
    "def all_metrics(predictions,truth):\n",
    "    return [f(predictions,truth) for f in [mae,mse,rmse]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mae\",\"mse\",\"rmse\"]\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results[\"metrics\"] = metrics\n",
    "results[\"mean_prediction\"] = all_metrics(X_test[\"mean_prediction\"],X_test[\"note\"])\n",
    "results[\"muser_prediction\"] = all_metrics(X_test[\"muser_prediction\"],X_test[\"note\"])\n",
    "results[\"mitem_prediction\"] = all_metrics(X_test[\"mitem_prediction\"],X_test[\"note\"])\n",
    "results = results.set_index(\"metrics\")\n",
    "\n",
    "print(results)\n",
    "print(\"\")\n",
    "print('---Best Models / Metrics: ---')\n",
    "results.idxmin(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this to install required packages if needed (and restart kernel !)\n",
    "#! pip install --upgrade scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor, BaselineOnly, SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(X_test[['author', 'title', 'note']], Reader(rating_scale=(1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaselineModel = BaselineOnly()\n",
    "BaselineModel.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_bl_rating_pred(user_item):\n",
    "    user = user_item[\"author\"]\n",
    "    item = user_item[\"title\"]\n",
    "    \n",
    "    prediction = BaselineModel.predict(user,item)\n",
    "    \n",
    "    return prediction.est\n",
    "\n",
    "X_test[\"opt_bl_prediction\"] = X_test[[\"author\",\"title\"]].apply(opt_bl_rating_pred,axis=1) \n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SVDmodel = SVD()\n",
    "SVDmodel.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
